As the amount of light actually received by a telescope is reduced by transmission through the Earth's atmosphere, any measurement of apparent magnitude is corrected for what it would have been as seen from above the atmosphere. The dimmer an object appears, the higher the numerical value given to its apparent magnitude, with a difference of 5 magnitudes corresponding to a brightness factor of exactly 100. Therefore, the apparent magnitude m, in the spectral band x, would be given by





m

x


=
−
5

log

100


⁡

(



F

x



F

x
,
0




)

,


{\displaystyle m_{x}=-5\log _{100}\left({\frac {F_{x}}{F_{x,0}}}\right),}

which is more commonly expressed in terms of common (base-10) logarithms as





m

x


=
−
2.5

log

10


⁡

(



F

x



F

x
,
0




)

,


{\displaystyle m_{x}=-2.5\log _{10}\left({\frac {F_{x}}{F_{x,0}}}\right),}

where Fx is the observed flux density using spectral filter x, and Fx,0 is the reference flux (zero-point) for that photometric filter. Since an increase of 5 magnitudes corresponds to a decrease in brightness by a factor of exactly 100, each magnitude increase implies a decrease in brightness by the factor 5√100 ≈ 2.512 (Pogson's ratio). Inverting the above formula, a magnitude difference m1 − m2 = Δm implies a brightness factor of







F

2



F

1




=

100



Δ
m

5



=

10

0.4
Δ
m


≈

2.512

Δ
m


.


{\displaystyle {\frac {F_{2}}{F_{1}}}=100^{\frac {\Delta m}{5}}=10^{0.4\Delta m}\approx 2.512^{\Delta m}.}