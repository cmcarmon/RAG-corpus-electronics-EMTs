The parametric test is accomplished by undertaking separate least squares analyses on two subsets of the original dataset: these subsets are specified so that the observations for which the pre-identified explanatory variable takes the lowest values are in one subset, with higher values in the other. The subsets needs not be of equal size, nor contain all the observations between them. The parametric test assumes that the errors have a normal distribution. There is an additional assumption here, that the design matrices for the two subsets of data are both of full rank. The test statistic used is the ratio of the mean square residual errors for the regressions on the two subsets. This test statistic corresponds to an F-test of equality of variances, and a one- or two-sided test may be appropriate depending on whether or not the direction of the supposed relation of the error variance to the explanatory variable is known.
Increasing the number of observations dropped in the "middle" of the ordering will increase the power of the test but reduce the degrees of freedom for the test statistic. As a result of this tradeoff it is common to see the Goldfeldâ€“Quandt test performed by dropping the middle third of observations with smaller proportions of dropped observations as sample size increases.