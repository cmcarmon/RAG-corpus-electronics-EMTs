In statistics the mean squared prediction error of a smoothing or curve fitting procedure is the expected value of the squared difference between the fitted values implied by the predictive function






g
^





{\displaystyle {\widehat {g}}}
and the values of the (unobservable) function g. It is an inverse measure of the explanatory power of






g
^



,


{\displaystyle {\widehat {g}},}
and can be used in the process of cross-validation of an estimated model.
If the smoothing or fitting procedure has operator matrix (i.e., hat matrix) L, which maps the observed values vector



y


{\displaystyle y}
to predicted values vector






y
^





{\displaystyle {\hat {y}}}
via






y
^



=
L
y
,


{\displaystyle {\hat {y}}=Ly,}
then




MSPE
⁡
(
L
)
=
E
⁡

[


(

g
(

x

i


)
−



g
^



(

x

i


)

)


2


]

.


{\displaystyle \operatorname {MSPE} (L)=\operatorname {E} \left[\left(g(x_{i})-{\widehat {g}}(x_{i})\right)^{2}\right].}

The MSPE can be decomposed into two terms (just like mean squared error is decomposed into bias and variance); however for MSPE one term is the sum of squared biases of the fitted values and another the sum of variances of the fitted values:




MSPE
⁡
(
L
)
=

∑

i
=
1


n




(

E
⁡

[




g
^



(

x

i


)

]

−
g
(

x

i


)

)


2


+

∑

i
=
1


n


var
⁡

[




g
^



(

x

i


)

]

.


{\displaystyle \operatorname {MSPE} (L)=\sum _{i=1}^{n}\left(\operatorname {E} \left[{\widehat {g}}(x_{i})\right]-g(x_{i})\right)^{2}+\sum _{i=1}^{n}\operatorname {var} \left[{\widehat {g}}(x_{i})\right].}

Knowledge of g is required in order to calculate MSPE exactly.