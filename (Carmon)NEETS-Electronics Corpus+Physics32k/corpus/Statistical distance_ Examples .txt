Some important statistical distances include the following:
f-divergence: includes
Kullback–Leibler divergence
Hellinger distance
Total variation distance (sometimes just called "the" statistical distance)

Rényi's divergence
Jensen–Shannon divergence
Lévy–Prokhorov metric
Bhattacharyya distance
Wasserstein metric: also known as the Kantorovich metric, or earth mover's distance
The Kolmogorov–Smirnov statistic represents a distance between two probability distributions defined on a single real variable
The maximum mean discrepancy which is defined in terms of the kernel embedding of distributions
Other approaches
Signal-to-noise ratio distance
Mahalanobis distance
Energy distance
Distance correlation is a measure of dependence between two random variables, it is zero if and only if the random variables are independent.

The continuous ranked probability score is a measure how good forecasts that are expressed as probability distributions are in matching observed outcomes. Both the location and spread of the forecast distribution are taken into account in judging how close the distribution is the observed value: see probabilistic forecasting.
Łukaszyk–Karmowski metric is a function defining a distance between two random variables or two random vectors. It does not satisfy the identity of indiscernibles condition of the metric and is zero if and only if both its arguments are certain events described by Dirac delta density probability distribution functions.