Many methods have been used to simulate mean solar time. The earliest were clepsydras or water clocks, used for almost four millennia from as early as the middle of the 2nd millennium BC until the early 2nd millennium. Before the middle of the 1st millennium BC, the water clocks were only adjusted to agree with the apparent solar day, thus were no better than the shadow cast by a gnomon (a vertical pole), except that they could be used at night.
But it has long been known that the Sun moves eastward relative to the fixed stars along the ecliptic. Since the middle of the first millennium BC the diurnal rotation of the fixed stars has been used to determine mean solar time, against which clocks were compared to determine their error rate. Babylonian astronomers knew of the equation of time and were correcting for it as well as the different rotation rate of the stars, sidereal time, to obtain a mean solar time much more accurate than their water clocks. This ideal mean solar time has been used ever since then to describe the motions of the planets, Moon, and Sun.
Mechanical clocks did not achieve the accuracy of Earth's "star clock" until the beginning of the 20th century. Today's atomic clocks have a much more constant rate than the Earth, but its star clock is still used to determine mean solar time. Since sometime in the late 20th century, Earth's rotation has been defined relative to an ensemble of extra-galactic radio sources and then converted to mean solar time by an adopted ratio. The difference between this calculated mean solar time and Coordinated Universal Time (UTC) determines whether a leap second is needed. (The UTC time scale now runs on SI seconds, and the SI second, when adopted, was already a little shorter than the current value of the second of mean solar time.)