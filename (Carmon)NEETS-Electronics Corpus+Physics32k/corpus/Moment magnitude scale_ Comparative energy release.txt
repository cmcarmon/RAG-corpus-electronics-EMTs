Assuming the values of





σ
¯



/

μ


{\displaystyle {\overline {\sigma }}/\mu }
are the same for all earthquakes, one can consider Mw as a measure of the potential energy change ΔW caused by earthquakes. Similarly, if one assumes




η

R


Δ

σ

s



/

2
μ


{\displaystyle \eta _{R}\Delta \sigma _{s}/2\mu }
is the same for all earthquakes, one can consider Mw as a measure of the energy Es radiated by earthquakes.
Under these assumptions, the following formula, obtained by solving for M0 the equation defining Mw, allows one to assess the ratio




E

1



/


E

2




{\displaystyle E_{1}/E_{2}}
of energy release (potential or radiated) between two earthquakes of different moment magnitudes,




m

1




{\displaystyle m_{1}}
and




m

2




{\displaystyle m_{2}}
:





E

1



/


E

2


≈

10



3
2


(

m

1


−

m

2


)


.


{\displaystyle E_{1}/E_{2}\approx 10^{{\frac {3}{2}}(m_{1}-m_{2})}.}

As with the Richter scale, an increase of one step on the logarithmic scale of moment magnitude corresponds to a 101.5 ≈ 32 times increase in the amount of energy released, and an increase of two steps corresponds to a 103 = 1000 times increase in energy. Thus, an earthquake of Mw of 7.0 contains 1000 times as much energy as one of 5.0 and about 32 times that of 6.0.