Reward-based algorithms assume that the robot in each state (position and internal state, including direction) can choose between different actions (motion). However, the result of each action is not definite. In other words, outcomes (displacement) are partly random and partly under the control of the robot. The robot gets positive reward when it reaches the target and gets negative reward if it collides with an obstacle. These algorithms try to find a path which maximizes cumulative future rewards. The Markov decision process (MDP) is a popular mathematical framework that is used in many reward-based algorithms. The advantage of MDPs over other reward-based algorithms is that they generate the optimal path. The disadvantage of MDPs is that they limit the robot to choose from a finite set of actions. Therefore, the path is not smooth (similar to grid-based approaches). Fuzzy Markov decision processes (FDMPs) are an extension of MDPs which generate smooth paths using a fuzzy inference system.