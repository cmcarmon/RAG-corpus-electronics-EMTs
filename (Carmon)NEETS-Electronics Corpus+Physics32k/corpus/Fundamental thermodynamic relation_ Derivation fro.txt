The above derivation uses the first and second laws of thermodynamics. The first law of thermodynamics is essentially a definition of heat, i.e. heat is the change in the internal energy of a system that is not caused by a change of the external parameters of the system.
However, the second law of thermodynamics is not a defining relation for the entropy. The fundamental definition of entropy of an isolated system containing an amount of energy of



E


{\displaystyle E}
is:




S
=
k
log
⁡

[

Ω

(
E
)


]




{\displaystyle S=k\log \left[\Omega \left(E\right)\right]\,}

where



Ω

(
E
)



{\displaystyle \Omega \left(E\right)}
is the number of quantum states in a small interval between



E


{\displaystyle E}
and



E
+
δ
E


{\displaystyle E+\delta E}
. Here



δ
E


{\displaystyle \delta E}
is a macroscopically small energy interval that is kept fixed. Strictly speaking this means that the entropy depends on the choice of



δ
E


{\displaystyle \delta E}
. However, in the thermodynamic limit (i.e. in the limit of infinitely large system size), the specific entropy (entropy per unit volume or per unit mass) does not depend on



δ
E


{\displaystyle \delta E}
. The entropy is thus a measure of the uncertainty about exactly which quantum state the system is in, given that we know its energy to be in some interval of size



δ
E


{\displaystyle \delta E}
.
Deriving the fundamental thermodynamic relation from first principles thus amounts to proving that the above definition of entropy implies that for reversible processes we have:




d
S
=



δ
Q

T




{\displaystyle dS={\frac {\delta Q}{T}}}

The fundamental assumption of statistical mechanics is that all the



Ω

(
E
)



{\displaystyle \Omega \left(E\right)}
states are equally likely. This allows us to extract all the thermodynamical quantities of interest. The temperature is defined as:






1

k
T



≡
β
≡



d
log
⁡

[

Ω

(
E
)


]



d
E






{\displaystyle {\frac {1}{kT}}\equiv \beta \equiv {\frac {d\log \left[\Omega \left(E\right)\right]}{dE}}\,}

This definition can be derived from the microcanonical ensemble, which is a system of a constant number of particles, a constant volume and that does not exchange energy with its environment. Suppose that the system has some external parameter, x, that can be changed. In general, the energy eigenstates of the system will depend on x. According to the adiabatic theorem of quantum mechanics, in the limit of an infinitely slow change of the system's Hamiltonian, the system will stay in the same energy eigenstate and thus change its energy according to the change in energy of the energy eigenstate it is in.
The generalized force, X, corresponding to the external parameter x is defined such that



X
d
x


{\displaystyle Xdx}
is the work performed by the system if x is increased by an amount dx. E.g., if x is the volume, then X is the pressure. The generalized force for a system known to be in energy eigenstate




E

r




{\displaystyle E_{r}}
is given by:




X
=
−



d

E

r




d
x





{\displaystyle X=-{\frac {dE_{r}}{dx}}}

Since the system can be in any energy eigenstate within an interval of



δ
E


{\displaystyle \delta E}
, we define the generalized force for the system as the expectation value of the above expression:




X
=
−

⟨



d

E

r




d
x



⟩




{\displaystyle X=-\left\langle {\frac {dE_{r}}{dx}}\right\rangle \,}

To evaluate the average, we partition the



Ω

(
E
)



{\displaystyle \Omega \left(E\right)}
energy eigenstates by counting how many of them have a value for






d

E

r




d
x





{\displaystyle {\frac {dE_{r}}{dx}}}
within a range between



Y


{\displaystyle Y}
and



Y
+
δ
Y


{\displaystyle Y+\delta Y}
. Calling this number




Ω

Y



(
E
)



{\displaystyle \Omega _{Y}\left(E\right)}
, we have:




Ω

(
E
)

=

∑

Y



Ω

Y



(
E
)




{\displaystyle \Omega \left(E\right)=\sum _{Y}\Omega _{Y}\left(E\right)\,}

The average defining the generalized force can now be written:




X
=
−


1

Ω

(
E
)





∑

Y


Y

Ω

Y



(
E
)




{\displaystyle X=-{\frac {1}{\Omega \left(E\right)}}\sum _{Y}Y\Omega _{Y}\left(E\right)\,}

We can relate this to the derivative of the entropy with respect to x at constant energy E as follows. Suppose we change x to x + dx. Then



Ω

(
E
)



{\displaystyle \Omega \left(E\right)}
will change because the energy eigenstates depend on x, causing energy eigenstates to move into or out of the range between



E


{\displaystyle E}
and



E
+
δ
E


{\displaystyle E+\delta E}
. Let's focus again on the energy eigenstates for which






d

E

r




d
x





{\displaystyle {\frac {dE_{r}}{dx}}}
lies within the range between



Y


{\displaystyle Y}
and



Y
+
δ
Y


{\displaystyle Y+\delta Y}
. Since these energy eigenstates increase in energy by Y dx, all such energy eigenstates that are in the interval ranging from E - Y dx to E move from below E to above E. There are





N

Y



(
E
)

=




Ω

Y



(
E
)



δ
E



Y
d
x



{\displaystyle N_{Y}\left(E\right)={\frac {\Omega _{Y}\left(E\right)}{\delta E}}Ydx\,}

such energy eigenstates. If



Y
d
x
≤
δ
E


{\displaystyle Ydx\leq \delta E}
, all these energy eigenstates will move into the range between



E


{\displaystyle E}
and



E
+
δ
E


{\displaystyle E+\delta E}
and contribute to an increase in



Ω


{\displaystyle \Omega }
. The number of energy eigenstates that move from below



E
+
δ
E


{\displaystyle E+\delta E}
to above



E
+
δ
E


{\displaystyle E+\delta E}
is, of course, given by




N

Y



(

E
+
δ
E

)



{\displaystyle N_{Y}\left(E+\delta E\right)}
. The difference





N

Y



(
E
)

−

N

Y



(

E
+
δ
E

)




{\displaystyle N_{Y}\left(E\right)-N_{Y}\left(E+\delta E\right)\,}

is thus the net contribution to the increase in



Ω


{\displaystyle \Omega }
. Note that if Y dx is larger than



δ
E


{\displaystyle \delta E}
there will be energy eigenstates that move from below



E


{\displaystyle E}
to above



E
+
δ
E


{\displaystyle E+\delta E}
. They are counted in both




N

Y



(
E
)



{\displaystyle N_{Y}\left(E\right)}
and




N

Y



(

E
+
δ
E

)



{\displaystyle N_{Y}\left(E+\delta E\right)}
, therefore the above expression is also valid in that case.
Expressing the above expression as a derivative with respect to E and summing over Y yields the expression:






(



∂
Ω


∂
x



)


E


=
−

∑

Y


Y


(



∂

Ω

Y




∂
E



)


x


=


(



∂

(

Ω
X

)



∂
E



)


x





{\displaystyle \left({\frac {\partial \Omega }{\partial x}}\right)_{E}=-\sum _{Y}Y\left({\frac {\partial \Omega _{Y}}{\partial E}}\right)_{x}=\left({\frac {\partial \left(\Omega X\right)}{\partial E}}\right)_{x}\,}

The logarithmic derivative of



Ω


{\displaystyle \Omega }
with respect to x is thus given by:






(



∂
log
⁡

(
Ω
)



∂
x



)


E


=
β
X
+


(



∂
X


∂
E



)


x





{\displaystyle \left({\frac {\partial \log \left(\Omega \right)}{\partial x}}\right)_{E}=\beta X+\left({\frac {\partial X}{\partial E}}\right)_{x}\,}

The first term is intensive, i.e. it does not scale with system size. In contrast, the last term scales as the inverse system size and thus vanishes in the thermodynamic limit. We have thus found that:






(



∂
S


∂
x



)


E


=


X
T





{\displaystyle \left({\frac {\partial S}{\partial x}}\right)_{E}={\frac {X}{T}}\,}

Combining this with






(



∂
S


∂
E



)


x


=


1
T





{\displaystyle \left({\frac {\partial S}{\partial E}}\right)_{x}={\frac {1}{T}}\,}

Gives:




d
S
=


(



∂
S


∂
E



)


x


d
E
+


(



∂
S


∂
x



)


E


d
x
=



d
E

T


+


X
T


d
x



{\displaystyle dS=\left({\frac {\partial S}{\partial E}}\right)_{x}dE+\left({\frac {\partial S}{\partial x}}\right)_{E}dx={\frac {dE}{T}}+{\frac {X}{T}}dx\,}

which we can write as:




d
E
=
T
d
S
−
X
d
x



{\displaystyle dE=TdS-Xdx\,}