The moment magnitude scale (MMS; denoted as Mw or M) is used by seismologists to measure the size of earthquakes.
The scale was developed in the 1970s to succeed the 1930s-era Richter magnitude scale (ML). Even though the formulas are different, the new scale retains a continuum of magnitude values similar to that defined by the older one. Under suitable assumptions, as with the Richter magnitude scale, an increase of one step on this logarithmic scale corresponds to a 101.5 (about 32) times increase in the amount of energy released, and an increase of two steps corresponds to a 103 (1,000) times increase in energy. Thus, an earthquake of Mw of 7.0 releases about 32 times as much energy as one of 6.0 and nearly 1,000 times that of 5.0.
The moment magnitude is based on the seismic moment of the earthquake, which is equal to the shear modulus of the rock near the fault multiplied by the average amount of slip on the fault and the size of the area that slipped.
Since January 2002, the MMS has been the scale used by the United States Geological Survey to calculate and report magnitudes for all modern large earthquakes.
Popular press reports of earthquake magnitude usually fail to distinguish between magnitude scales, and are often reported as "Richter magnitudes" when the reported magnitude is a moment magnitude (or a surface-wave or body-wave magnitude). Because the scales are intended to report the same results within their applicable conditions, the confusion is minor.