Time-dependent perturbation theory, developed by Paul Dirac, studies the effect of a time-dependent perturbation V(t) applied to a time-independent Hamiltonian H0.
Since the perturbed Hamiltonian is time-dependent, so are its energy levels and eigenstates. Thus, the goals of time-dependent perturbation theory are slightly different from time-independent perturbation theory. One is interested in the following quantities:
The time-dependent expectation value of some observable A, for a given initial state.
The time-dependent amplitudes of those quantum states that are energy eigenkets (eigenvectors) in the unperturbed system.
The first quantity is important because it gives rise to the classical result of an A measurement performed on a macroscopic number of copies of the perturbed system. For example, we could take A to be the displacement in the x-direction of the electron in a hydrogen atom, in which case the expected value, when multiplied by an appropriate coefficient, gives the time-dependent dielectric polarization of a hydrogen gas. With an appropriate choice of perturbation (i.e. an oscillating electric potential), this allows one to calculate the AC permittivity of the gas.
The second quantity looks at the time-dependent probability of occupation for each eigenstate. This is particularly useful in laser physics, where one is interested in the populations of different atomic states in a gas when a time-dependent electric field is applied. These probabilities are also useful for calculating the "quantum broadening" of spectral lines (see line broadening) and particle decay in particle physics and nuclear physics.
We will briefly examine the method behind Dirac's formulation of time-dependent perturbation theory. Choose an energy basis





|

n
⟩



{\displaystyle {|n\rangle }}
for the unperturbed system. (We drop the (0) superscripts for the eigenstates, because it is not useful to speak of energy levels and eigenstates for the perturbed system.)
If the unperturbed system is in eigenstate (of the Hamiltonian)




|

j
⟩


{\displaystyle |j\rangle }
at time t = 0, its state at subsequent times varies only by a phase (in the Schrödinger picture, where state vectors evolve in time and operators are constant),





|

j
(
t
)
⟩
=

e

−
i

E

j


t

/

ℏ



|

j
⟩

.


{\displaystyle |j(t)\rangle =e^{-iE_{j}t/\hbar }|j\rangle ~.}

Now, introduce a time-dependent perturbing Hamiltonian V(t). The Hamiltonian of the perturbed system is




H
=

H

0


+
V
(
t
)

.


{\displaystyle H=H_{0}+V(t)~.}

Let




|

ψ
(
t
)
⟩


{\displaystyle |\psi (t)\rangle }
denote the quantum state of the perturbed system at time t. It obeys the time-dependent Schrödinger equation,




H

|

ψ
(
t
)
⟩
=
i
ℏ


∂

∂
t




|

ψ
(
t
)
⟩

.


{\displaystyle H|\psi (t)\rangle =i\hbar {\frac {\partial }{\partial t}}|\psi (t)\rangle ~.}

The quantum state at each instant can be expressed as a linear combination of the complete eigenbasis of




|

n
⟩


{\displaystyle |n\rangle }
:

where the cn(t)s are to be determined complex functions of t which we will refer to as amplitudes (strictly speaking, they are the amplitudes in the Dirac picture).
We have explicitly extracted the exponential phase factors



exp
⁡
(
−
i

E

n


t

/

ℏ
)


{\displaystyle \exp(-iE_{n}t/\hbar )}
on the right hand side. This is only a matter of convention, and may be done without loss of generality. The reason we go to this trouble is that when the system starts in the state




|

j
⟩


{\displaystyle |j\rangle }
and no perturbation is present, the amplitudes have the convenient property that, for all t, cj(t) = 1 and cn(t) = 0 if n ≠ j.
The square of the absolute amplitude cn(t) is the probability that the system is in state n at time t, since






|


c

n


(
t
)

|


2


=


|

⟨
n

|

ψ
(
t
)
⟩

|


2



.


{\displaystyle \left|c_{n}(t)\right|^{2}=\left|\langle n|\psi (t)\rangle \right|^{2}~.}

Plugging into the Schrödinger equation and using the fact that ∂/∂t acts by a product rule, one obtains





∑

n



(

i
ℏ



∂

c

n




∂
t



−

c

n


(
t
)
V
(
t
)

)


e

−
i

E

n


t

/

ℏ



|

n
⟩
=
0

.


{\displaystyle \sum _{n}\left(i\hbar {\frac {\partial c_{n}}{\partial t}}-c_{n}(t)V(t)\right)e^{-iE_{n}t/\hbar }|n\rangle =0~.}

By resolving the identity in front of V and multiplying through by the bra



⟨
n

|



{\displaystyle \langle n|}
on the left, this can be reduced to a set of coupled differential equations for the amplitudes,







∂

c

n




∂
t



=



−
i

ℏ



∑

k


⟨
n

|

V
(
t
)

|

k
⟩


c

k


(
t
)


e

−
i
(

E

k


−

E

n


)
t

/

ℏ



.


{\displaystyle {\frac {\partial c_{n}}{\partial t}}={\frac {-i}{\hbar }}\sum _{k}\langle n|V(t)|k\rangle \,c_{k}(t)\,e^{-i(E_{k}-E_{n})t/\hbar }~.}

where we have used equation (1) to evaluate the sum on n in the second term, then used the fact that



⟨
k

|

Ψ
(
t
)
⟩
=

c

k


(
t
)

e

−
i

E

k


t

/

ℏ




{\displaystyle \langle k|\Psi (t)\rangle =c_{k}(t)e^{-iE_{k}t/\hbar }}
.
The matrix elements of V play a similar role as in time-independent perturbation theory, being proportional to the rate at which amplitudes are shifted between states. Note, however, that the direction of the shift is modified by the exponential phase factor. Over times much longer than the energy difference Ek − En, the phase winds around 0 several times. If the time-dependence of V is sufficiently slow, this may cause the state amplitudes to oscillate. ( E.g., such oscillations are useful for managing radiative transitions in a laser.)
Up to this point, we have made no approximations, so this set of differential equations is exact. By supplying appropriate initial values cn(t), we could in principle find an exact (i.e., non-perturbative) solution. This is easily done when there are only two energy levels (n = 1, 2), and this solution is useful for modelling systems like the ammonia molecule.
However, exact solutions are difficult to find when there are many energy levels, and one instead looks for perturbative solutions. These may be obtained by expressing the equations in an integral form,





c

n


(
t
)
=

c

n


(
0
)
+



−
i

ℏ



∑

k



∫

0


t


d

t
′


⟨
n

|

V
(

t
′

)

|

k
⟩


c

k


(

t
′

)


e

−
i
(

E

k


−

E

n


)

t
′


/

ℏ



.


{\displaystyle c_{n}(t)=c_{n}(0)+{\frac {-i}{\hbar }}\sum _{k}\int _{0}^{t}dt'\;\langle n|V(t')|k\rangle \,c_{k}(t')\,e^{-i(E_{k}-E_{n})t'/\hbar }~.}

Repeatedly substituting this expression for cn back into right hand side, yields an iterative solution,





c

n


(
t
)
=

c

n


(
0
)


+

c

n


(
1
)


+

c

n


(
2
)


+
⋯


{\displaystyle c_{n}(t)=c_{n}^{(0)}+c_{n}^{(1)}+c_{n}^{(2)}+\cdots }

where, for example, the first-order term is





c

n


(
1
)


(
t
)
=



−
i

ℏ



∑

k



∫

0


t


d

t
′


⟨
n

|

V
(

t
′

)

|

k
⟩


c

k


(
0
)


e

−
i
(

E

k


−

E

n


)

t
′


/

ℏ



.


{\displaystyle c_{n}^{(1)}(t)={\frac {-i}{\hbar }}\sum _{k}\int _{0}^{t}dt'\;\langle n|V(t')|k\rangle \,c_{k}(0)\,e^{-i(E_{k}-E_{n})t'/\hbar }~.}

Several further results follow from this, such as Fermi's golden rule, which relates the rate of transitions between quantum states to the density of states at particular energies; or the Dyson series, obtained by applying the iterative method to the time evolution operator, which is one of the starting points for the method of Feynman diagrams.