In 1935, seismologists Charles Francis Richter and Beno Gutenberg of the California Institute of Technology developed a scale, later dubbed the Richter magnitude scale, for computing the magnitude of earthquakes, specifically those recorded and measured with the Wood-Anderson torsion seismograph in a particular area of California. Originally, Richter reported mathematical values to the nearest quarter of a unit, but the values later were reported with one decimal place; the local magnitude scale compared the magnitudes of different earthquakes. Richter derived his earthquake-magnitude scale from the apparent magnitude scale used to measure the brightness of stars.
Richter established a magnitude 0 event to be an earthquake that would show a maximum, combined horizontal displacement of 1.0 μm (3.9×10−5 in) on a seismogram recorded with a Wood-Anderson torsion seismograph 100 km (62 mi) from the earthquake epicenter. That fixed measure was chosen to avoid negative values for magnitude, given that the slightest earthquakes that could be recorded and located at the time were around magnitude 3.0. The Richter magnitude scale itself has no lower limit, and contemporary seismometers can register, record, and measure earthquakes with negative magnitudes.





M

L




{\displaystyle M_{\text{L}}}
(local magnitude) was not designed to be applied to data with distances to the hypocenter of the earthquake that were greater than 600 km (370 mi). For national and local seismological observatories, the standard magnitude scale in the 21st century is still




M

L




{\displaystyle M_{\text{L}}}
. However, this scale cannot measure magnitudes above about




M

L




{\displaystyle M_{\text{L}}}
= 7, because the high frequency waves recorded locally have wavelengths shorter than the rupture lengths of large earthquakes.
Later, to express the size of earthquakes around the planet, Gutenberg and Richter developed a surface wave magnitude scale (




M

s




{\displaystyle M_{\text{s}}}
) and a body wave magnitude scale (




M

b




{\displaystyle M_{\text{b}}}
). These are types of waves that are recorded at teleseismic distances. The two scales were adjusted such that they were consistent with the




M

L




{\displaystyle M_{\text{L}}}
scale. That adjustment succeeded better with the




M

s




{\displaystyle M_{\text{s}}}
scale than with the




M

b




{\displaystyle M_{\text{b}}}
scale. Each scale saturates when the earthquake is greater than magnitude 8.0.
Because of this, researchers in the 1970s developed the moment magnitude scale (




M

w




{\displaystyle M_{\text{w}}}
). The older magnitude-scales were superseded by methods for calculating the seismic moment, from which was derived the moment magnitude scale.
About the origins of the Richter magnitude scale, C. F. Richter said:

I found a [1928] paper by Professor K. Wadati of Japan in which he compared large earthquakes by plotting the maximum ground motion against [the] distance to the epicenter. I tried a similar procedure for our stations, but the range between the largest and smallest magnitudes seemed unmanageably large. Dr. Beno Gutenberg then made the natural suggestion to plot the amplitudes logarithmically. I was lucky, because logarithmic plots are a device of the devil.