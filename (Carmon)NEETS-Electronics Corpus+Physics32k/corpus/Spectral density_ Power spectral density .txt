The above definition of energy spectral density is suitable for transients (pulse-like signals) whose energy is concentrated around one time window; then the Fourier transforms of the signals generally exist. For continuous signals over all time, such as stationary processes, one must rather define the power spectral density (PSD); this describes how power of a signal or time series is distributed over frequency, as in the simple example given previously. Here, power can be the actual physical power, or more often, for convenience with abstract signals, is simply identified with the squared value of the signal. For example, statisticians study the variance of a function over time



x
(
t
)


{\displaystyle x(t)}
(or over another independent variable), and using an analogy with electrical signals (among other physical processes), it is customary to refer to it as the power spectrum even when there is no physical power involved. If one were to create a physical voltage source which followed



x
(
t
)


{\displaystyle x(t)}
and applied it to the terminals of a 1 ohm resistor, then indeed the instantaneous power dissipated in that resistor would be given by x2 watts.
The average power P of a signal



x
(
t
)


{\displaystyle x(t)}
over all time is therefore given by the following time average:




P
=

lim

T
→
∞




1

2
T




∫

−
T


T



|

x
(
t
)


|


2



d
t
.


{\displaystyle P=\lim _{T\to \infty }{\frac {1}{2T}}\int _{-T}^{T}|x(t)|^{2}\,dt.}

Note that a stationary process, for instance, may have a finite power but an infinite energy. After all, energy is the integral of power, and the stationary signal continues over an infinite time. That is the reason that we cannot use the energy spectral density as defined above in such cases.
In analyzing the frequency content of the signal



x
(
t
)


{\displaystyle x(t)}
, one might like to compute the ordinary Fourier transform






x
^



(
ω
)


{\displaystyle {\hat {x}}(\omega )}
; however, for many signals of interest the Fourier transform does not formally exist. Because of this complication one can as well work with a truncated Fourier transform







x
^




T


(
ω
)


{\displaystyle {\hat {x}}_{T}(\omega )}
, where the signal is integrated only over a finite interval [0, T]:








x
^




T


(
ω
)
=


1

T




∫

0


T


x
(
t
)

e

−
i
ω
t



d
t
.


{\displaystyle {\hat {x}}_{T}(\omega )={\frac {1}{\sqrt {T}}}\int _{0}^{T}x(t)e^{-i\omega t}\,dt.}

Then the power spectral density can be defined as





S

x
x


(
ω
)
=

lim

T
→
∞



E


[


|





x
^




T


(
ω
)

|


2


]

.


{\displaystyle S_{xx}(\omega )=\lim _{T\to \infty }\mathbf {E} \left[\left|{\hat {x}}_{T}(\omega )\right|^{2}\right].}

Here E denotes the expected value; explicitly, we have





E


[


|





x
^




T


(
ω
)

|


2


]

=

E


[



1
T



∫

0


T



x

∗


(
t
)

e

i
ω
t



d
t

∫

0


T


x
(

t
′

)

e

−
i
ω

t
′




d

t
′


]

=


1
T



∫

0


T



∫

0


T



E


[


x

∗


(
t
)
x
(

t
′

)

]


e

i
ω
(
t
−

t
′

)



d
t

d

t
′

.


{\displaystyle \mathbf {E} \left[\left|{\hat {x}}_{T}(\omega )\right|^{2}\right]=\mathbf {E} \left[{\frac {1}{T}}\int _{0}^{T}x^{*}(t)e^{i\omega t}\,dt\int _{0}^{T}x(t')e^{-i\omega t'}\,dt'\right]={\frac {1}{T}}\int _{0}^{T}\int _{0}^{T}\mathbf {E} \left[x^{*}(t)x(t')\right]e^{i\omega (t-t')}\,dt\,dt'.}

In the latter form (for a stationary random process), one can make the change of variables



Δ
t
=
t
−

t
′



{\displaystyle \Delta t=t-t'}
and with the limits of integration (rather than [0,T]) approaching infinity, the resulting power spectral density




S

x
x


(
ω
)


{\displaystyle S_{xx}(\omega )}
and the autocorrelation function of this signal are seen to be Fourier transform pairs (Wiener–Khinchin theorem). The autocorrelation function is a statistic defined as




γ
(
τ
)
=
⟨
X
(
t
)
X
(
t
+
τ
)
⟩
=

E

[
X
(
t
)
X
(
t
+
τ
)
]


{\displaystyle \gamma (\tau )=\langle X(t)X(t+\tau )\rangle =\mathbf {E} [X(t)X(t+\tau )]}

or more generally as




γ
(
τ
)
=
⟨
X
(
t
)
X
(
t
−
τ

)

∗


⟩
=
⟨
X
(
t

)

∗


X
(
t
+
τ
)
⟩


{\displaystyle \gamma (\tau )=\langle X(t)X(t-\tau )^{*}\rangle =\langle X(t)^{*}X(t+\tau )\rangle }

in the case that X(t) is complex-valued. Provided that



γ
(
τ
)


{\displaystyle \gamma (\tau )}
is absolutely integrable (which is not always true),





S

x
x


(
ω
)
=

∫

−
∞


∞


γ
(
τ
)

e

−
i
ω
τ



d
τ
=



γ
^



(
ω
)
.


{\displaystyle S_{xx}(\omega )=\int _{-\infty }^{\infty }\gamma (\tau )e^{-i\omega \tau }\,d\tau ={\hat {\gamma }}(\omega ).}

Many authors use this equality to actually define the power spectral density.
The power of the signal in a given frequency band



[

f

1


,

f

2


]


{\displaystyle [f_{1},f_{2}]}
(or



[

ω

1


,

ω

2


]


{\displaystyle [\omega _{1},\omega _{2}]}
) can be calculated by integrating over frequency. Since




S

x
x


(
−
ω
)
=

S

x
x


(
ω
)


{\displaystyle S_{xx}(-\omega )=S_{xx}(\omega )}
, an equal amount of power can be attributed to positive and negative frequencies, which accounts for the factor of 2 in the following form (such trivial factors dependent on conventions used):





P


b
a
n
d
l
i
m
i
t
e
d



=
2

∫


f

1





f

2





S

x
x


(
2
π

f
)

d
f
=


1
π



∫


ω

1





ω

2





S

x
x


(
ω
)
d
ω


{\displaystyle P_{\mathsf {bandlimited}}=2\int _{f_{1}}^{f_{2}}S_{xx}(2\pi \!f)\,df={\frac {1}{\pi }}\int _{\omega _{1}}^{\omega _{2}}S_{xx}(\omega )d\omega }

More generally, similar techniques may be used to estimate a time-varying spectral density. In this case the truncated Fourier transform defined above over the finite time interval (0, T) is not evaluated in the limit of T approaching infinity. This results in decreased spectral coverage and resolution since frequencies of less than 1/T are not sampled, and results at frequencies which are not an integer multiple of 1/T are not independent. Just using a single such time series, the estimated power spectrum will be very "noisy", however this can be alleviated if it is possible to evaluate the expected value (in the above equation) using a large (or infinite) number of short-term spectra corresponding to statistical ensembles of realizations of x(t) evaluated over the specified time window.
This definition of the power spectral density can be generalized to discrete time variables




x

n




{\displaystyle x_{n}}
. As above we can consider a finite window of



1
≤
n
≤
N


{\displaystyle 1\leq n\leq N}
with the signal sampled at discrete times




x

n


=
x
(
n
Δ
t
)


{\displaystyle x_{n}=x(n\Delta t)}
for a total measurement period



T
=
N
Δ
t


{\displaystyle T=N\Delta t}
. Then a single estimate of the PSD can be obtained through summation rather than integration:








S
~




x
x


(
ω
)
=



(
Δ
t

)

2



T




|


∑

n
=
1


N



x

n



e

−
i
ω
n



|


2




{\displaystyle {\tilde {S}}_{xx}(\omega )={\frac {(\Delta t)^{2}}{T}}\left|\sum _{n=1}^{N}x_{n}e^{-i\omega n}\right|^{2}}
.
As before, the actual PSD is achieved when N (and thus T) approach infinity and the expected value is formally applied. In a real-world application, one would typically average this single-measurement PSD over many trials to obtain a more accurate estimate of the theoretical PSD of the physical process underlying the individual measurements. This computed PSD is sometimes called a periodogram. This periodogram converges to the true PSD as the number of estimates as well as the averaging time interval T approach infinity (Brown & Hwang).
If two signals both possess power spectral densities, then the #Cross-spectral density can similarly be calculated; as the PSD is related to the autocorrelation, so is the cross-spectral density related to the cross-correlation.