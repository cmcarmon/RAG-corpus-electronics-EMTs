As an important special case, an easy to use recursive expression can be derived when at each t-th time instant the underlying linear observation process yields a scalar such that




y

t


=

a

t


T



x

t


+

z

t




{\displaystyle y_{t}=a_{t}^{T}x_{t}+z_{t}}
, where




a

t


T




{\displaystyle a_{t}^{T}}
is 1-by-n known row vector whose values can change with time,




x

t




{\displaystyle x_{t}}
is n-by-1 random column vector to be estimated, and




z

t




{\displaystyle z_{t}}
is scalar noise term with variance




σ

t


2




{\displaystyle \sigma _{t}^{2}}
. After (t+1)-th observation, the direct use of above recursive equations give the expression for the estimate







x
^




t
+
1




{\displaystyle {\hat {x}}_{t+1}}
as:








x
^




t
+
1


=




x
^




t


+

k

t
+
1


(

y

t
+
1


−

a

t
+
1


T






x
^




t


)


{\displaystyle {\hat {x}}_{t+1}={\hat {x}}_{t}+k_{t+1}(y_{t+1}-a_{t+1}^{T}{\hat {x}}_{t})}

where




y

t
+
1




{\displaystyle y_{t+1}}
is the new scalar observation and the gain factor




k

t
+
1




{\displaystyle k_{t+1}}
is n-by-1 column vector given by





k

t
+
1


=



(

C

e



)

t



a

t
+
1





σ

t
+
1


2


+

a

t
+
1


T


(

C

e



)

t



a

t
+
1





.


{\displaystyle k_{t+1}={\frac {(C_{e})_{t}a_{t+1}}{\sigma _{t+1}^{2}+a_{t+1}^{T}(C_{e})_{t}a_{t+1}}}.}

The



(

C

e



)

t
+
1




{\displaystyle (C_{e})_{t+1}}
is n-by-n error covariance matrix given by




(

C

e



)

t
+
1


=
(
I
−

k

t
+
1



a

t
+
1


T


)
(

C

e



)

t


.


{\displaystyle (C_{e})_{t+1}=(I-k_{t+1}a_{t+1}^{T})(C_{e})_{t}.}

Here no matrix inversion is required. Also the gain factor




k

t
+
1




{\displaystyle k_{t+1}}
depends on our confidence in the new data sample, as measured by the noise variance, versus that in the previous data. The initial values of






x
^





{\displaystyle {\hat {x}}}
and




C

e




{\displaystyle C_{e}}
are taken to be the mean and covariance of the aprior probability density function of



x


{\displaystyle x}
.
This important special case has also given rise to many other iterative methods (or adaptive filters), such as the least mean squares filter and recursive least squares filter, that directly solves the original MSE optimization problem using gradient descent methods. These methods bypass the need for covariance matrices.