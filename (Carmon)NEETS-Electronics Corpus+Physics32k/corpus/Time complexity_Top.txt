In computer science, the time complexity is the computational complexity that measures or estimates the time taken for running an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that an elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm differ by at most a constant factor.
Since an algorithm's running time may vary with different inputs of the same size, one commonly considers the worst-case time complexity, which is the maximum amount of time taken on inputs of a given size. Less common, and usually specified explicitly, is the average-case complexity, which is the average of the time taken on inputs of a given size (this makes sense, as there is only a finite number of possible inputs of a given size).
In both cases, the time complexity is generally expressed as a function of the size of the input. Since this function is generally difficult to compute exactly, and the running time is usually not critical for small input, one focuses commonly on the behavior of the complexity when the input size increases; that is, on the asymptotic behavior of the complexity. Therefore, the time complexity is commonly expressed using big O notation, typically



O
(
n
)
,


{\displaystyle O(n),}




O
(
n
log
⁡
n
)
,


{\displaystyle O(n\log n),}




O
(

n

α


)
,


{\displaystyle O(n^{\alpha }),}




O
(

2

n


)
,


{\displaystyle O(2^{n}),}
etc., where n is the input size measured by the number of bits needed for representing it.
Algorithm complexities are classified by the function appearing in the big O notation. For example, an algorithm with time complexity



O
(
n
)


{\displaystyle O(n)}
is a linear time algorithm, an algorithm with time complexity



O
(

n

α


)


{\displaystyle O(n^{\alpha })}
for some constant



α
≥
1


{\displaystyle \alpha \geq 1}
is a polynomial time algorithm.