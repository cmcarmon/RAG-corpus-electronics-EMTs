Luminosity is an intrinsic measurable property of a star independent of distance. The concept of magnitude, on the other hand, incorporates distance. First conceived by the Greek astronomer Hipparchus in the second century BC, the original concept of magnitude grouped stars into six discrete categories depending on how bright they appeared. The brightest first magnitude stars were twice as bright as the next brightest stars, which were second magnitude; second was twice as bright as third, third twice as bright as fourth and so on down to the faintest stars, which Hipparchus categorized as sixth magnitude. The system was but a simple delineation of stellar brightness into six distinct groups and made no allowance for the variations in brightness within a group. With the invention of the telescope at the beginning of the seventeenth century, researchers soon realized that there were subtle variations among stars and millions fainter than the sixth magnitude—hence the need for a more sophisticated system to describe a continuous range of values beyond what the naked eye could see.
In 1856 Norman Pogson, noticing that photometric measurements had established first magnitude stars as being about 100 times brighter than sixth magnitude stars, formalized the Hipparchus system by creating a logarithmic scale, with every interval of one magnitude equating to a variation in brightness of 1001/5 or roughly 2.512 times. Consequently, a first magnitude star is about 2.5 times brighter than a second magnitude star, 2.52 brighter than a third magnitude star, 2.53 brighter than a fourth magnitude star, et cetera. Based on this continuous scale, any star with a magnitude between 5.5 and 6.5 is now considered to be sixth magnitude, a star with a magnitude between 4.5 and 5.5 is fifth magnitude and so on. With this new mathematical rigor, a first magnitude star should then have a magnitude in the range 0.5 to 1.5, thus excluding the nine brightest stars with magnitudes lower than 0.5, as well as the four brightest with negative values. It is customary therefore to extend the definition of a first magnitude star to any star with a magnitude less than 0.5, as can be seen in accompanying table.

The Pogson logarithmic scale is used to measure both apparent and absolute magnitudes, the latter corresponding to the brightness of a star or other celestial body as seen if it would be located at an interstellar distance of 10 parsecs. The apparent magnitude is a measure of the diminishing flux of light as a result of distance according to the inverse-square law. In addition to this brightness decrease from increased distance, there is an extra decrease of brightness due to extinction from intervening interstellar dust.
By measuring the width of certain absorption lines in the stellar spectrum, it is often possible to assign a certain luminosity class to a star without knowing its distance. Thus a fair measure of its absolute magnitude can be determined without knowing its distance nor the interstellar extinction, allowing astronomers to estimate a star's distance and extinction without parallax calculations. Since the stellar parallax is usually too small to be measured for many distant stars, this is a common method of determining such distances.
To conceptualize the range of magnitudes in our own galaxy, the smallest star to be identified has about 8% of the Sun’s mass and glows feebly at absolute magnitude +19. Compared to the Sun, which has an absolute of +4.8, this faint star is 14 magnitudes or 400,000 times dimmer than our Sun. Our galaxy's most massive stars begin their lives with masses of roughly 100 times solar, radiating at upwards of absolute magnitude –8, over 160,000 times the solar luminosity. The total range of stellar luminosities, then, occupies a range of 27 magnitudes, or a factor of 60 billion.
In measuring star brightnesses, absolute magnitude, apparent magnitude, and distance are interrelated parameters—if two are known, the third can be determined. Since the Sun's luminosity is the standard, comparing these parameters with the Sun's apparent magnitude and distance is the easiest way to remember how to convert between them.