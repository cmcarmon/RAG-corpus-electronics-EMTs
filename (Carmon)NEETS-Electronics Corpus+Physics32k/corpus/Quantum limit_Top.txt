A quantum limit in physics is a limit on measurement accuracy at quantum scales. Depending on the context, the limit may be absolute (such as the Heisenberg limit), or it may only apply when the experiment is conducted with naturally occurring quantum states (e.g. the standard quantum limit in interferometry) and can be circumvented with advanced state preparation and measurement schemes.
The usage of the term standard quantum limit or SQL is, however, broader than just interferometry. In principle, any linear measurement of a quantum mechanical observable of a system under study that does not commute with itself at different times leads to such limits. In short, it is the Heisenberg uncertainty principle that is the cause .

A more detailed explanation would be that any measurement in quantum mechanics involves at least two parties, an Object and a Meter. The former is the system whose observable, say






x
^





{\displaystyle {\hat {x}}}
, we want to measure. The latter is the system we couple to the Object in order to infer the value of






x
^





{\displaystyle {\hat {x}}}
of the Object by recording some chosen observable,







O

^





{\displaystyle {\hat {\mathcal {O}}}}
, of this system, e.g. the position of the pointer on a scale of the Meter. This, in a nutshell, is a model of most of the measurements happening in physics, known as indirect measurements (see pp. 38–42 of ). So any measurement is a result of interaction and that acts in both ways. Therefore, the Meter acts on the Object during each measurement, usually via the quantity,







F

^





{\displaystyle {\hat {\mathcal {F}}}}
, conjugate to the readout observable







O

^





{\displaystyle {\hat {\mathcal {O}}}}
, thus perturbing the value of measured observable






x
^





{\displaystyle {\hat {x}}}
and modifying the results of subsequent measurements. This is known as back action (quantum) of the Meter on the system under measurement.
At the same time, quantum mechanics prescribes that readout observable of the Meter should have an inherent uncertainty,



δ




O

^





{\displaystyle \delta {\hat {\mathcal {O}}}}
, additive to and independent of the value of the measured quantity






x
^





{\displaystyle {\hat {x}}}
. This one is known as measurement imprecision or measurement noise. Because of the Heisenberg uncertainty principle, this imprecision cannot be arbitrary and is linked to the back-action perturbation by the uncertainty relation:




Δ


O


Δ


F


⩾
ℏ

/

2

,


{\displaystyle \Delta {\mathcal {O}}\Delta {\mathcal {F}}\geqslant \hbar /2\,,}

where



Δ
a
=


⟨




a
^




2


⟩
−
⟨



a
^




⟩

2






{\displaystyle \Delta a={\sqrt {\langle {\hat {a}}^{2}\rangle -\langle {\hat {a}}\rangle ^{2}}}}
is a standard deviation of observable



a


{\displaystyle a}
and



⟨



a
^



⟩


{\displaystyle \langle {\hat {a}}\rangle }
stands for expectation value of



a


{\displaystyle a}
in whatever quantum state the system is. The equality is reached if the system is in a minimum uncertainty state. The consequence for our case is that the more precise is our measurement, i.e the smaller is



Δ


δ
O




{\displaystyle \Delta {\mathcal {\delta O}}}
, the larger will be perturbation the Meter exerts on the measured observable






x
^





{\displaystyle {\hat {x}}}
. Therefore, the readout of the meter will, in general, consist of three terms:








O

^



=




x
^





f
r
e
e



+
δ




O

^



+
δ




x
^




B
A


[




F

^



]

,


{\displaystyle {\hat {\mathcal {O}}}={\hat {x}}_{\mathrm {free} }+\delta {\hat {\mathcal {O}}}+\delta {\hat {x}}_{BA}[{\hat {\mathcal {F}}}]\,,}

where







x
^





f
r
e
e





{\displaystyle {\hat {x}}_{\mathrm {free} }}
is a value of






x
^





{\displaystyle {\hat {x}}}
that the Object would have, were it not coupled to the Meter, and



δ




x

B
A


^



[




F

^



]


{\displaystyle \delta {\hat {x_{BA}}}[{\hat {\mathcal {F}}}]}
is the perturbation to the value of






x
^





{\displaystyle {\hat {x}}}
caused by back action force,







F

^





{\displaystyle {\hat {\mathcal {F}}}}
. The uncertainty of the latter is proportional to



Δ


F


∝
Δ



O



−
1




{\displaystyle \Delta {\mathcal {F}}\propto \Delta {\mathcal {O}}^{-1}}
. Thus, there is a minimal value, or the limit to the precision one can get in such a measurement, provided that



δ




O

^





{\displaystyle \delta {\hat {\mathcal {O}}}}
and







F

^





{\displaystyle {\hat {\mathcal {F}}}}
are uncorrelated  .
The terms "quantum limit" and "standard quantum limit" are sometimes used interchangeably. Usually, "quantum limit" is a general term which refers to any restriction on measurement due to quantum effects, while the "standard quantum limit" in any given context refers to a quantum limit which is ubiquitous in that context.