Another issue is bit errors induced by the digital channel; if they occur (and they will), synchronization can
easily be lost even if the receiver started “in synch” with the source. Despite the small probabilities of error
oﬀered by good signal set design and the matched ﬁlter, an infrequent error can devastate the ability to
translate a bitstream into a symbolic signal. We need ways of reducing reception errors without demanding
that pe be smaller.
Example 6.4
The ﬁrst electrical communications system—the telegraph—was digital. When ﬁrst deployed in
1844, it communicated text over wireline connections using a binary code—the Morse code—to
represent individual letters. To send a message from one place to another, telegraph operators
would tap the message using a telegraph key to another operator, who would relay the message
on to the next operator, presumably getting the message closer to its destination. In short, the
telegraph relied on a network not unlike the basics of modern computer networks. To say it
presaged modern communications would be an understatement.
It was also far ahead of some
needed technologies, namely the Source Coding Theorem. The Morse code, shown in Figure 6.19,
was not a preﬁx code. To separate codes for each letter, Morse code required that a space—a
pause—be inserted between each letter. In information theory, that space counts as another code
letter, which means that the Morse code encoded text with a three-letter source code: dots, dashes
and space. The resulting source code is not within a bit of entropy, and is grossly ineﬃcient (about
25%). Figure 6.19 shows a Huﬀman code for English text, which as we know is eﬃcient.