The second is the International System of Units (SI) unit of time duration. It is also the standard single-unit time representation in many programming languages, most notably C, and part of UNIX/POSIX standards used by Linux, Mac OS X, etc.; to convert fractional days to fractional seconds, multiply the number by 86400. Fractional seconds are represented as milliseconds (ms), microseconds (Î¼s) or nanoseconds (ns). Absolute times are usually represented relative to 1 January 1970, at midnight. Other systems may use a different zero point (a.k.a. Epoch).
In principle, time spans greater than one second are given in units such as kiloseconds (ks), myriasecond (mys), megaseconds (Ms), gigaseconds (Gs), and so on. (The myriasecond is based on the myria- prefix, which represented a multiple of 10000 but was made obsolete in the mid-20th century.) Occasionally, these units can be found in technical literature, but traditional units like minutes, hours, days and years are much more common, and are accepted for use with SI.
It is possible to specify the time of day as the number of kiloseconds or myriaseconds elapsed since midnight. Thus, instead of saying 3:45 p.m. one could say (time of day) 56.7 ks. There are exactly 86.4 ks or 8.64 mys in one day. However, this nomenclature is rarely used in practice.