The magnetic moment is defined as a vector relating the aligning torque on the object from an externally applied magnetic field to the field vector itself. The relationship is given by:





τ

=

μ

×

B



{\displaystyle {\boldsymbol {\tau }}={\boldsymbol {\mu }}\times \mathbf {B} }

where τ is the torque acting on the dipole, B is the external magnetic field, and μ is the magnetic moment.
This definition is based on how one would measure the magnetic moment, in principle, of an unknown sample if possible.