The bit is the basic unit of information. It is used to represent information by computers. Regardless of its physical realization, a bit has two possible states typically thought of as 0 and 1, but more generally—and according to applications—interpretable as false and true respectively, or any other dichotomous choice. An analogy to this is a light switch—its OFF position can be thought of as 0 and its ON position as 1.
Ultimately though, within classic computer technology, a processed bit is a question of two levels of low DC voltage, and whilst switching from one of these two levels to the other, a so-called forbidden zone must be passed as fast as possible, as electrical voltage cannot change from one level to another instantaneously. This was a problem during the childhood of the binary chip.
A qubit has a few similarities to a classical bit, but is overall very different. There are two possible outcomes for the measurement of a qubit—usually 0 and 1, like a bit. The difference is that whereas the state of a bit is either 0 or 1, the state of a qubit can also be a superposition of both. It is possible to fully encode one bit in one qubit. However, a qubit can hold even more information, e.g. up to two bits using superdense coding.
For a system of n components, a complete description of its state in classical physics requires only n bits, whereas in quantum physics it requires 2n−1 complex numbers.