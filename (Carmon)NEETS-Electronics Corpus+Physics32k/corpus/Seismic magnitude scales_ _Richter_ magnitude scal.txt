The first scale for measuring earthquake magnitudes, developed in 1935 by Charles F. Richter and popularly known as the "Richter" scale, is actually the Local magnitude scale, label ML or ML. Richter established two features now common to all magnitude scales. First, the scale is logarithmic, so that each unit represents a ten-fold increase in the amplitude of the seismic waves. As the energy of a wave is 101.5 times its amplitude, each unit of magnitude represents a nearly 32-fold increase in the energy (strength) of an earthquake.
Second, Richter arbitrarily defined the zero point of the scale to be where an earthquake at a distance of 100 km makes a maximum horizontal displacement of 0.001 millimeters (1 Âµm, or 0.00004 in.) on a seismogram recorded with a Wood-Anderson torsion seismograph. Subsequent magnitude scales are calibrated to be approximately in accord with the original "Richter" (local) scale around magnitude 6.
All "Local" (ML) magnitudes are based on the maximum amplitude of the ground shaking, without distinguishing the different seismic waves. They underestimate the strength:
of distant earthquakes (over ~600 km) because of attenuation of the S-waves,
of deep earthquakes because the surface waves are smaller, and
of strong earthquakes (over M ~7) because they do not take into account the duration of shaking.
The original "Richter" scale, developed in the geological context of Southern California and Nevada, was later found to be inaccurate for earthquakes in the central and eastern parts of the continent (everywhere east of the Rocky Mountains) because of differences in the continental crust. All these problems prompted development of other scales.
Most seismological authorities, such as the United States Geological Survey, report earthquake magnitudes above 4.0 as moment magnitude (below), which the press describes as "Richter magnitude".