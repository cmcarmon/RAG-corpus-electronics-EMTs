Assuming the Earth is a perfect sphere with no terrain irregularity, the line-of-sight distance to the horizon from a transmitter located at a given altitude above the surface can readily be calculated. Let R be the radius of Earth and h be the altitude of the transmitter's antenna. Line of sight distance d of this station is given by the Pythagorean theorem;





d

2


=
(
R
+
h

)

2


−

R

2


=
2
⋅
R
⋅
h
+

h

2




{\displaystyle d^{2}=(R+h)^{2}-R^{2}=2\cdot R\cdot h+h^{2}}

Since the altitude of the station is much less than the radius of the Earth,




d
≈


2
⋅
R
⋅
h




{\displaystyle d\approx {\sqrt {2\cdot R\cdot h}}}

The mean radius of the earth is about 6,378 kilometres (3,963 mi). (See Earth radius) Using the same units for both the altitude of the station and the radius of the earth,




d
≈


2
⋅
6378
⋅
h


≈
112.9
⋅


h




{\displaystyle d\approx {\sqrt {2\cdot 6378\cdot h}}\approx 112.9\cdot {\sqrt {h}}}

If the height is given in m. and distance in km.




d
≈
3.57
⋅


h




{\displaystyle d\approx 3.57\cdot {\sqrt {h}}}

If the height is given in ft. and the distance in miles,




d
≈
1.23
⋅


h




{\displaystyle d\approx 1.23\cdot {\sqrt {h}}}

Of course the presence of hills or mountains between the transmitter and receiver may reduce this distance.