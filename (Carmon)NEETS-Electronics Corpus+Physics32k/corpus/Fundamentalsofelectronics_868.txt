The code thus obtained is not unique as we could have labeled the branches coming out of
each node diﬀerently. The average number of bits required to represent this alphabet equals
1.75 bits, which is the Shannon entropy limit for this source alphabet. If we had the symbolic-
valued signal s (m) = {a2, a3, a1, a4, a1, a2, . . .}, our Huﬀman code would produce the bitstream
b (n) = 101100111010 . . ..