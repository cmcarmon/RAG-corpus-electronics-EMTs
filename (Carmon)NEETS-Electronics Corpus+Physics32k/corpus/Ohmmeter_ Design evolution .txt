The first ohmmeters were based on a type of meter movement known as a 'ratiometer'. These were similar to the galvanometer type movement encountered in later instruments, but instead of hairsprings to supply a restoring force they used conducting 'ligaments'. These provided no net rotational force to the movement. Also, the movement was wound with two coils. One was connected via a series resistor to the battery supply. The second was connected to the same battery supply via a second resistor and the resistor under test. The indication on the meter was proportional to the ratio of the currents through the two coils. This ratio was determined by the magnitude of the resistor under test. The advantages of this arrangement were twofold. First, the indication of the resistance was completely independent of the battery voltage (as long as it actually produced some voltage) and no zero adjustment was required. Second, although the resistance scale was non linear, the scale remained correct over the full deflection range. By interchanging the two coils a second range was provided. This scale was reversed compared to the first. A feature of this type of instrument was that it would continue to indicate a random resistance value once the test leads were disconnected (the action of which disconnected the battery from the movement). Ohmmeters of this type only ever measured resistance as they could not easily be incorporated into a multimeter design. Insulation testers that relied on a hand cranked generator operated on the same principle. This ensured that the indication was wholly independent of the voltage actually produced.
Subsequent designs of ohmmeter provided a small battery to apply a voltage to a resistance via a galvanometer to measure the current through the resistance (battery, galvanometer and resistance all connected in series). The scale of the galvanometer was marked in ohms, because the fixed voltage from the battery assured that as resistance is increased, the current through the meter (and hence deflection) would decrease. Ohmmeters form circuits by themselves, therefore they cannot be used within an assembled circuit. This design is much simpler and cheaper than the former design, and was simple to integrate into a multimeter design and consequently was by far the most common form of analogue ohmmeter. This type of ohmmeter suffers from two inherent disadvantages. First, the meter needs to be zeroed by shorting the measurement points together and performing an adjustment for zero ohms indication prior to each measurement. This is because as the battery voltage decreases with age, the series resistance in the meter needs to be reduced to maintain the zero indication at full deflection. Second, and consequent on the first, the actual deflection for any given resistor under test changes as the internal resistance is altered. It remains correct at the centre of the scale only, which is why such ohmmeter designs always quote the accuracy "at centre scale only".
A more accurate type of ohmmeter has an electronic circuit that passes a constant current (I) through the resistance, and another circuit that measures the voltage (V) across the resistance. These measurements are then digitized with an analog digital converter (adc) after which a microcontroller or microprocessor make the division of the current and voltage according to Ohm's Law and then decode these to a display to offer the user a reading of the resistance value they're measuring at that instant. Since these type of meters already measure current,voltage and resistance all at once, these type of circuits are often used in digital multimeters.