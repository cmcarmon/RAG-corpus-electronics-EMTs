rarely, if ever, are concerned with basic sub-word phonetic
elements, and seldom do they deal with or higher level abstrac-
tions like subtle nuances of meaning. In contrast, an LM often
reaches deep into the acoustic structure of speech and some-
times high into the more abstract aspects of intention and
meaning. In essence, what primary school students know as
‘‘grammar’’ is the natural-language version of only one of the
ingredients of a formal LM used in speech recognition. A LM is
a broader framework, consisting of the (often quantiﬁed)
formalization of some or all of the structure that grammarians,
linguists, psychologists, and others who study language can
ascribe to a natural language. Indeed, an LM need not conform
to the grammatical rules of the natural language it is modeling.
Peirce (Liszka, 1996) identiﬁes four components of the nat-
ural-language code: symbolic, grammatical,19 semantic, and
pragmatic. Implicit or explicit banks of linguistic knowledge
resident in speech recognizers, sometimes called knowledge
sources (Reddy, 1976), can usually be associated with a com-
ponent of Peirce’s model. The symbols of a language are deﬁned
to be the most fundamental units from which all messages are
ultimately composed. In the spoken form of a language, for
example, the symbols might be words or phonemes, whereas in
the written form, the alphabet might serve as the symbols. For
the purposes of this discussion, let us consider the phonemes to
be the symbols of a natural language. The grammar of the
language is concerned with how symbols are related to one
another to form message units. If we consider the sentence to
be the ultimate message unit, then how words are formed from
phonemes are part of Peirce’s grammar as well as the manner in
which words form sentences. How phonemes form words is
governed by lexical constraints and how words form sentences
by syntactic constraints. Lexical and syntactic constraints are
both components of the grammar.